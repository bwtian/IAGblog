---
title: 20190515爬虫
author: Tian 
date: '2019-05-15'
slug: 20190515postscrape
categories: []
tags: []
---

# 爬取政府报告2019
http://www.gov.cn/premier/2019-03/16/content_5374314.htm

```{r}
x <- 1:10
y <- 51:60
plot(x,y)

```

## 爬网页
首先爬，接着统计、最后可视化

```{r scrape}
library(rvest)
link <- "http://www.gov.cn/premier/2019-03/16/content_5374314.htm"
web<-read_html(link,encoding="utf-8") #读取数据，规定编码
position<-web %>% html_nodes("div.pages_content") %>% html_text()
print(position)

library(jiebaR)
engine_s<-worker()#初始化分词引擎并加载停用词。
seg<-segment(position,engine_s)#分词
f1 <- freq(seg)
f1<-f1[order(f1[2],decreasing=TRUE),] #根据词频降序排列
f1
print(f1)
#engine_s1<-worker(stop_word = "stopwords.txt")
#seg1<-segment(position,engine_s1)
#f1 <- freq(seg1)
#f1<-f1[order(f1[2],decreasing=TRUE),]
#print(f1)
library(wordcloud2)#加载包
f2<-f1[1:150,]     #总共有2000多个词，为了显示效果，我只提取前150个字
wordcloud2(f2, size = 0.8 ,shape='star')    #形状设置为一颗五角星
getwd()
```


